#!/usr/bin/python -u

import sys, os, os.path, string, glob, math
import random
import time
import datetime
import tempfile
import subprocess
import unittest
import argparse
import StringIO


# If we're using this directly from the albatross repo, we can find repo modules here:
if sys.path[0] != '':
    modules_dir = '%s/../modules' % (sys.path[0])
else:
    modules_dir='../modules'
sys.path += [modules_dir]

# import albatross repo modules
import utcondor


# large scale test
class grid_scale_test_large(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # I expect this from repo submodule condor_tools
        if sys.path[0] != '':
            self.ctbin = "%s/../submodules/condor_tools/bin"%(sys.path[0])
        else:
            self.ctbin = "../submodules/condor_tools/bin"

        self.tmpdir = tempfile.mkdtemp(prefix='sh_large_')
        sys.stdout.write("working-directory= %s\n" % self.tmpdir)

        self.ntarget = 10
        self.n_startd = 50
        self.n_slots = 20
        self.n_dynamic = 0
        self.n_schedd = 10
        
        # class specific setup goes after parent class
        candidate_nodes = self.candidate_nodes(without_any_feats=['CentralManager','Negotiator','Collector'])
        candidate_nodes = list(set(candidate_nodes)-set([self.hostname]))

        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestLarge')
        self.build_access_feature('GridScaleTestLargeAccess')
        (pslots,dslots) = self.build_execute_feature('GridScaleTestLargeExecute', n_startd=self.n_startd, n_slots=self.n_slots, n_dynamic=self.n_dynamic, dl_append=False)

        self.build_feature('GridScaleTestLargePorts', params={"LOWPORT":"1024", "HIGHPORT":"64000"})

        self.build_feature('GridScaleTestLargeUpdate', params={"UPDATE_INTERVAL":"60"})
        #self.build_feature('GridScaleTestLargeBenchOff', params={"RUNBENCHMARKS":"FALSE"})

        # define features on the given groups
        self.assert_group_features(utcondor.reverse(['NodeAccess', 'Master', 'GridScaleTestLarge', 'GridScaleTestLargeAccess', 'GridScaleTestLargeExecute', 'GridScaleTestLargeUpdate', 'GridScaleTestLargePorts']), ['GridScaleTestLarge'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestLarge', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)
        self.clear_default_group()

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestLarge'], self.target_nodes)

        # configure schedds
        schedd_names = self.build_scheduler_feature('GridScaleTestLargeSchedd', n_schedd=self.n_schedd)
        self.schedd_names = ["%s@%s" % (x, self.params.collector_addr) for x in schedd_names]

        # make sure I can fetch logs/history from CM
        self.build_feature('GridScaleTestLargeFetch', params={"ALLOW_ADMINISTRATOR":">= %s"%(self.hostname), "MAX_HISTORY_LOG":"1000000000"})

        # turn off plugins
        self.build_feature('GridScaleTestLargeNoPlugins', params={"MASTER.PLUGINS":"", "SCHEDD.PLUGINS":"", "COLLECTOR.PLUGINS":"", "NEGOTIATOR.PLUGINS":"", "STARTD.PLUGINS":""})

        # miscellaneous settings
        self.build_feature('GridScaleTestLargeNeg', params={"NEGOTIATOR_INTERVAL":"30", "NEGOTIATOR_MAX_TIME_PER_SUBMITTER":"31536000", "NEGOTIATOR_DEBUG":"", "MAX_NEGOTIATOR_LOG":"100000000", "SCHEDD_DEBUG":"", "MAX_SCHEDD_LOG":"100000000", "COLLECTOR_DEBUG":"", "SHADOW_LOCK":"", "SHADOW_LOG":"", "NEGOTIATOR_PRE_JOB_RANK":"0", "NEGOTIATOR_POST_JOB_RANK":"0"})

        self.build_feature('GridScaleTestLargeNoPreempt', params={"NEGOTIATOR_CONSIDER_PREEMPTION":"FALSE", "PREEMPTION_REQUIREMENTS":"FALSE", "RANK":"0", "SHADOW_TIMEOUT_MULTIPLIER":"4", "SHADOW_WORKLIFE":"36000"})

        # In the current scheme I only want to add the schedds to existing CM, instead of ground-up CM configuration
        self.assert_node_features(utcondor.reverse(['NodeAccess', 'Master', 'GridScaleTestLargeAccess', 'GridScaleTestLargeSchedd', 'GridScaleTestLargeFetch', 'GridScaleTestLargeNeg', 'GridScaleTestLargeNoPlugins', 'GridScaleTestLargeNoPreempt', 'GridScaleTestLargePorts']), [self.params.collector_addr], mod_op='insert')

        self.build_feature('GridScaleTestLargeIP', params={"CONDOR_HOST":"10.16.43.33", "COLLECTOR_HOST":"10.16.43.33"})
        self.assert_node_features(['GridScaleTestLargeIP', 'GridScaleTestLargePorts'], [self.hostname], mod_op='insert')

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_large_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        try:
            self.poll_for_slots(self.ntarget*pslots, group='GridScaleTestLarge', interval=30, maxtime=900, expected_nodes=self.target_nodes, required=int(0.9*(self.ntarget*pslots)))
        except:
            pass
        else:
            # flag that all setup succeeded
            self.setup = True


    def tearDown(self):
        sys.stdout.write("working-directory= %s\n" % self.tmpdir)
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()

        sustain = 330
        nsub = 300
        interval = 1.0
        duration = 60

        submit_procs = []
        sincetime=time.time()
        for j in xrange(nsub):
            schedd_name = self.schedd_names[j % self.n_schedd]
            cjs_command = "%s/cjs -shell -dir '%s' -duration %d -xgroups U%03d 1 -reqs 'stringListMember(\"GridScaleTestLarge\", WallabyGroups) && (TARGET.Arch =!= UNDEFINED) && (TARGET.OpSys =!= UNDEFINED) && (TARGET.Disk >= 0) && (TARGET.Memory >= 0) && (TARGET.FileSystemDomain =!= UNDEFINED)' -ss -ss-interval %f -ss-maxtime %d -append '+CondorUnitTestTag=\"Large\"' -name '%s' >'%s/sh_out%03d' 2>'%s/sh_err%03d'" % (self.ctbin, self.tmpdir, duration, j, interval, sustain, schedd_name, self.tmpdir, j, self.tmpdir, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        elapsed = self.poll_for_process_completion(submit_procs)

        njobs = self.job_count(tag="Large", schedd=self.schedd_names)
        sys.stdout.write("elapsed time= %s  njobs= %d  sustained rate= %f  submitters= %d\n" % (elapsed, njobs, float(njobs)/float(elapsed), nsub))

        #self.remove_jobs(tag="Large", schedd=self.schedd_names)
        self.poll_for_empty_job_queue(tag="Large", interval=30, maxtime=3600, schedd=self.schedd_names)

        time.sleep(60)

        hfname = "%s/history" % (self.tmpdir)
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        hofsub = "%s/hofsub.dat" % (self.tmpdir)
        sys.stdout.write("submissions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -hof-out %s"%(self.ctbin, hfname, int(sincetime), hofsub)])
        sys.stdout.write("submissions rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -rate -hof %s"%(self.ctbin, hfname, int(sincetime), hofsub)])
        sys.stdout.write("submissions cum:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -cum -hof %s"%(self.ctbin, hfname, int(sincetime), hofsub)])
        sys.stdout.write("submissions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -cum -rate -hof %s"%(self.ctbin, hfname, int(sincetime), hofsub)])

        hofcpl = "%s/hofcpl.dat" % (self.tmpdir)
        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -hof-out %s"%(self.ctbin, hfname, int(sincetime), hofcpl)])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate -hof %s"%(self.ctbin, hfname, int(sincetime), hofcpl)])


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()

        duration = 30
        nsub = 500
        njob = 10000

        cjs_command = "%s/cjs -dir '%s' -duration %d -n %d -sub %d -reqs 'stringListMember(\"GridScaleTestLarge\", WallabyGroups)' -append '+CondorUnitTestTag=\"Large\"' >'%s/sh_out' 2>'%s/sh_err'" % (self.ctbin, self.tmpdir, duration, njob, nsub, self.tmpdir, self.tmpdir)

        sincetime=time.time()
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Large", interval = 60, maxtime=1800)

        hfname = "%s/cr_history" % (self.tmpdir)
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(self.ctbin, hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(self.ctbin, hfname, int(sincetime))])


# inherit standard args from utcondor
ha_parser = argparse.ArgumentParser(parents=[utcondor.parser])
ha_parser.add_argument('--setup-only', action='store_true', default=False, help='run test setup only: skip tests and do not restore config')
# Tentatively, I don't think it's a good idea to run all test cases
# So I'm making this a required positional param
ha_parser.add_argument('test_name')

# parse args from command line
args = ha_parser.parse_args()

# initialize utcondor params
if args.setup_only: args.no_restore = True
utcondor.init(args)

unittest.main(argv=[sys.argv[0], args.test_name])
