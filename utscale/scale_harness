#!/usr/bin/python

import sys, os, os.path, string, glob, math
import random
import time
import datetime
import tempfile
import subprocess
import unittest
import argparse
import StringIO

from wallabyclient.exceptions import *
from wallabyclient import WallabyHelpers, WallabyTypes
from qmf.console import Session

# If we're using this directly from the albatross repo, we can find repo modules here:
if sys.path[0] != '':
    modules_dir = '%s/../modules' % (sys.path[0])
else:
    modules_dir='../modules'
sys.path += [modules_dir]

# import albatross repo modules
import utcondor


# A prototype "micro" scale test, to run on a personal condor
class grid_scale_test_micro(utcondor.condor_unit_test):
    def setUp(self):
        # parent class setup first:
        utcondor.condor_unit_test.setUp(self)

        if len(self.node_names) < 1: raise Exception("Require at least one node in pool")
        target_node = self.node_names[0]
        sys.stdout.write("Target for test is: %s\n" % (target_node))

        self.assert_feature('GridScaleTestMicro')

        # define features on the given groups
        self.assert_group_features(['GridScaleTestMicro'], ['GridScaleTestMicro'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestMicro', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes([target_node])

        # define groups on the given nodes
        # For this 'micro' test, I'm assuming a personal condor that has
        # a configuration independent of wallaby -- in general, this would
        # require me to add basic functionality features, since I just cleared
        # everything above
        self.assert_node_groups(['GridScaleTestMicro'], [target_node])

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_micro_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        self.poll_for_slots(1, group='GridScaleTestMicro', interval=10, maxtime=120)


    def tearDown(self):
        # Don't wait for jobs to complete, just nuke them
        subprocess.call(["condor_rm", "-constraint", "CondorUnitTestTag==\"Micro\""], stdout=self.devnull, stderr=self.devnull)
        # Do wait for job queue to empty before restoration and reactivation
        self.poll_for_empty_job_queue(tag="Micro", interval=10, maxtime=120)

        # call base-class teardown after our class-specific work
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        # this unit test should pass
        n = 100
        sys.stdout.write("Testing submission rate over %d individual submits:\n" % (n))
        t0 = time.time()
        for k in xrange(n):
            subprocess.Popen("condor_submit", stdin=subprocess.PIPE, stdout=self.devnull, stderr=self.devnull).communicate(input='universe = vanilla\nexecutable = /bin/sleep\narguments = 10m\nrequirements = (WallabyGroups == "GridScaleTestMicro")\n+CondorUnitTestTag="Micro"\nqueue\n')
        elapsed = time.time() - t0
        sys.stdout.write("%f seconds for %d submits -> %f submissions / sec\n" % (elapsed, n, float(n)/float(elapsed)))
        


# small scale test
class grid_scale_test_small(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # class specific setup goes after parent class
        provisioned_nodes = self.list_nodes(without_any_feats=['CentralManager','Negotiator','Collector'], checkin_since=time.time()-4000)
        reporting_nodes = self.reporting_nodes()
        candidate_nodes = list(set(provisioned_nodes) & set(reporting_nodes))

        self.ntarget = 8
        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestSmall')
        self.build_access_feature('GridScaleTestSmallAccess')
        self.build_execute_feature('GridScaleTestSmallExecute', n_startd=8, n_slots=8, n_dynamic=0)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTestSmall', 'GridScaleTestSmallAccess', 'GridScaleTestSmallExecute'], ['GridScaleTestSmall'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestSmall', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestSmall'], self.target_nodes)

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_small_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        self.poll_for_slots(self.ntarget*8*8, group='GridScaleTestSmall', interval=10, maxtime=60, required=(self.ntarget-1)*8*8, expected_nodes=self.target_nodes)

        # flag that all setup succeeded
        self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        duration = 400
        maxreps = 330
        nsub = 10
        submit_procs = []
        for j in xrange(nsub):
            cjs_command = "~/git/condor_tools/bin/cjs -duration %d -xgroups U%03d 1 -reqs 'stringListMember(\"GridScaleTestSmall\", WallabyGroups)' -ss -ss-interval 0.95 -ss-maxreps %d >/tmp/sh_out%03d 2>/tmp/sh_err%03d" % (duration, j, maxreps, j, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        t0 = time.time()
        while True:
            time.sleep(1)
            completed = True
            for p in submit_procs:
                if p.poll() == None: completed = False
            if completed: break

        elapsed = time.time() - t0
        sys.stdout.write("elapsed time = %s  sustained rate = %f  with %d submitters\n" % (elapsed, float(maxreps)/float(elapsed), nsub))


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        cjs_command = "~/git/condor_tools/bin/cjs -duration 15 -n 4000 -sub 10 -reqs 'stringListMember(\"GridScaleTestSmall\", WallabyGroups)' -append '+CondorUnitTestTag=\"Small\"' >/tmp/sh_out 2>/tmp/sh_err"
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Small", interval = 60, maxtime=600)



# medium scale test
class grid_scale_test_medium(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # class specific setup goes after parent class
        qualified_nodes = self.list_nodes(without_any_feats=['CentralManager','Negotiator','Collector'], checkin_since=time.time()-4000)
        reporting_nodes = self.reporting_nodes()
        candidate_nodes = list(set(qualified_nodes) & set(reporting_nodes))

        self.ntarget = 10
        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            sys.stderr.write("qualified but not reporting= %s\n" % (list(set(qualified_nodes) - set(reporting_nodes))))
            sys.stderr.write("reporting but not qualified= %s\n" % (list(set(reporting_nodes) - set(qualified_nodes))))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestMedium')
        self.build_access_feature('GridScaleTestMediumAccess')
        (pslots,dslots) = self.build_execute_feature('GridScaleTestMediumExecute', n_startd=50, n_slots=1, n_dynamic=8)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTestMedium', 'GridScaleTestMediumAccess', 'GridScaleTestMediumExecute'], ['GridScaleTestMedium'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestMedium', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestMedium'], self.target_nodes)

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_medium_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        try:
            self.poll_for_slots(self.ntarget*pslots, group='GridScaleTestMedium', interval=15, maxtime=300, expected_nodes=self.target_nodes)
        except:
            pass
        else:
            # flag that all setup succeeded
            self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        duration = 15
        sustain = 300
        nsub = 20
        interval = 0.0
        submit_procs = []
        sincetime=time.time()
        for j in xrange(nsub):
            cjs_command = "~/git/condor_tools/bin/cjs -duration %d -xgroups U%03d 1 -reqs 'stringListMember(\"GridScaleTestMedium\", WallabyGroups)' -ss -ss-interval %f -ss-maxtime %d -log -append '+CondorUnitTestTag=\"Medium\"' >/tmp/sh_out%03d 2>/tmp/sh_err%03d" % (duration, j, interval, sustain, j, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        elapsed = self.poll_for_process_completion(submit_procs)

        njobs = self.job_count(tag="Medium")
        sys.stdout.write("elapsed time = %s  sustained rate = %f  with %d submitters\n" % (elapsed, float(njobs)/float(elapsed), nsub))

        self.remove_jobs(tag="Medium")
        self.poll_for_empty_job_queue(tag="Medium", interval=15, maxtime=3600)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("submissions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions"%(hfname, int(sincetime))])
        sys.stdout.write("submissions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -cum -rate"%(hfname, int(sincetime))])
        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(hfname, int(sincetime))])


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        cjs_command = "~/git/condor_tools/bin/cjs -duration 30 -n 10000 -sub 20 -reqs 'stringListMember(\"GridScaleTestMedium\", WallabyGroups)' -append '+CondorUnitTestTag=\"Medium\"' >/tmp/sh_out 2>/tmp/sh_err"

        sincetime=time.time()
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Medium", interval = 60, maxtime=3600)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(hfname, int(sincetime))])


# large scale test
class grid_scale_test_large(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        self.ntarget = 10
        self.n_startd = 50
        self.n_slots = 1
        self.n_dynamic = 8
        self.n_schedd = 25
        
        # class specific setup goes after parent class
        reporting_nodes = self.reporting_nodes(with_attr=['Machine', 'CondorPlatform'])
        reporting_nodes = [x[0] for x in reporting_nodes if (x[1].rfind('LINUX') >= 0)]
        qualified_nodes = self.list_nodes(without_any_feats=['CentralManager','Negotiator','Collector'], checkin_since=time.time()-4000)
        candidate_nodes = list((set(qualified_nodes) & set(reporting_nodes))-set([self.hostname]))

        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            sys.stderr.write("qualified but not reporting= %s\n" % (list(set(qualified_nodes) - set(reporting_nodes))))
            sys.stderr.write("reporting but not qualified= %s\n" % (list(set(reporting_nodes) - set(qualified_nodes))))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestLarge')
        self.build_access_feature('GridScaleTestLargeAccess')
        (pslots,dslots) = self.build_execute_feature('GridScaleTestLargeExecute', n_startd=self.n_startd, n_slots=self.n_slots, n_dynamic=self.n_dynamic, dl_append=False)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTestLarge', 'GridScaleTestLargeAccess', 'GridScaleTestLargeExecute'], ['GridScaleTestLarge'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestLarge', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestLarge'], self.target_nodes)

        # configure schedds
        self.build_scheduler_feature('GridScaleTestLargeSchedd', n_schedd=self.n_schedd)
        self.schedd_names = ["SCH%03d@%s" % (j, self.params.collector_addr) for j in xrange(self.n_schedd)]

        # make sure I can fetch logs/history from CM
        self.build_feature('GridScaleTestLargeFetch', params={"ALLOW_ADMINISTRATOR":">= %s"%(self.hostname), "MAX_HISTORY_LOG":"1000000000"})

        # miscellaneous settings
        self.build_feature('GridScaleTestLargeNeg', params={"NEGOTIATOR_INTERVAL":"300", "SCHEDD_DEBUG":"D_COMMAND", "MAX_SCHEDD_LOG":"100000000"})

        # In the current scheme I only want to add the schedds to existing CM, instead of ground-up CM configuration
        self.assert_node_features(['GridScaleTestLargeSchedd', 'GridScaleTestLargeFetch', 'GridScaleTestLargeNeg'], [self.params.collector_addr], mod_op='insert')

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_large_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        try:
            self.poll_for_slots(self.ntarget*pslots, group='GridScaleTestLarge', interval=30, maxtime=600, expected_nodes=self.target_nodes, required=int(0.9*(self.ntarget*pslots)))
        except:
            pass
        else:
            # flag that all setup succeeded
            self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        sustain = 330
        nsub = 100
        interval = 0.2
        duration = 360
        submit_procs = []
        sincetime=time.time()
        for j in xrange(nsub):
            schedd_name = self.schedd_names[j % self.n_schedd]
            cjs_command = "~/git/condor_tools/bin/cjs -duration %d -xgroups U%03d 1 -reqs 'FALSE && stringListMember(\"GridScaleTestLarge\", WallabyGroups)' -ss -ss-interval %f -ss-maxtime %d -append '+CondorUnitTestTag=\"Large\"' -name '%s' >/tmp/sh_out%03d 2>/tmp/sh_err%03d" % (duration, j, interval, sustain, schedd_name, j, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        elapsed = self.poll_for_process_completion(submit_procs)

        njobs = self.job_count(tag="Large", schedd=self.schedd_names)
        sys.stdout.write("elapsed time= %s  njobs= %d  sustained rate= %f  submitters= %d\n" % (elapsed, njobs, float(njobs)/float(elapsed), nsub))

        self.remove_jobs(tag="Large", schedd=self.schedd_names)
        self.poll_for_empty_job_queue(tag="Large", interval=30, maxtime=3600, schedd=self.schedd_names)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("submissions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions"%(hfname, int(sincetime))])
        sys.stdout.write("submissions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -cum -rate"%(hfname, int(sincetime))])
        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(hfname, int(sincetime))])


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        cjs_command = "~/git/condor_tools/bin/cjs -duration 30 -n 10000 -sub 20 -reqs 'stringListMember(\"GridScaleTestLarge\", WallabyGroups)' -append '+CondorUnitTestTag=\"Large\"' >/tmp/sh_out 2>/tmp/sh_err"

        sincetime=time.time()
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Large", interval = 60, maxtime=3600)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "~/git/condor_tools/bin/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(hfname, int(sincetime))])



class grid_scale_test_4000(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # class specific setup goes after parent class
        candidate_nodes = self.list_nodes(with_all_groups=['GridScaleTestNode'], without_any_feats=['CentralManager','Negotiator','Collector'])

        self.ntarget = 10
        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTest4000')
        self.build_execute_feature('GridScaleTest4000Execute', n_startd=50, n_slots=1, n_dynamic=8)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTest4000', 'GridScaleTest4000Execute'], ['GridScaleTest4000'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTest4000', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTest4000'], self.target_nodes)

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_4000_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        #self.poll_for_slots(self.ntarget*8*8, group='GridScaleTest4000', interval=30, maxtime=600)

        # flag that all setup succeeded
        self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_dummy(self):
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()


if __name__ == "__main__":
    # inherit standard args from utcondor
    ha_parser = argparse.ArgumentParser(parents=[utcondor.parser])
    ha_parser.add_argument('--setup-only', action='store_true', default=False, help='run test setup only: skip tests and do not restore config')
    # Tentatively, I don't think it's a good idea to run all test cases
    # So I'm making this a required positional param
    ha_parser.add_argument('test_name')

    # parse args from command line
    args = ha_parser.parse_args()

    # initialize utcondor params
    if args.setup_only: args.no_restore = True
    utcondor.init(args)

    # This provides nice default command-line facilities for running
    # any unit tests defined in this module/command.
    unittest.main(argv=[sys.argv[0], args.test_name])
