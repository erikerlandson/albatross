#!/usr/bin/python -u

import sys, os, os.path, string, glob, math
import random
import time
import datetime
import tempfile
import subprocess
import unittest
import argparse
import StringIO


# If we're using this directly from the albatross repo, we can find repo modules here:
if sys.path[0] != '':
    modules_dir = '%s/../modules' % (sys.path[0])
else:
    modules_dir='../modules'
sys.path += [modules_dir]

# import albatross repo modules
import utcondor


# A prototype "micro" scale test, to run on a personal condor
class grid_scale_test_micro(utcondor.condor_unit_test):
    def setUp(self):
        # parent class setup first:
        utcondor.condor_unit_test.setUp(self)

        if len(self.node_names) < 1: raise Exception("Require at least one node in pool")
        target_node = self.node_names[0]
        sys.stdout.write("Target for test is: %s\n" % (target_node))

        self.assert_feature('GridScaleTestMicro')

        # define features on the given groups
        self.assert_group_features(['GridScaleTestMicro'], ['GridScaleTestMicro'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestMicro', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes([target_node])
        self.clear_default_group()

        # define groups on the given nodes
        # For this 'micro' test, I'm assuming a personal condor that has
        # a configuration independent of wallaby -- in general, this would
        # require me to add basic functionality features, since I just cleared
        # everything above
        self.assert_node_groups(['GridScaleTestMicro'], [target_node])

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_micro_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        self.poll_for_slots(1, group='GridScaleTestMicro', interval=10, maxtime=120)


    def tearDown(self):
        # Don't wait for jobs to complete, just nuke them
        subprocess.call(["condor_rm", "-constraint", "CondorUnitTestTag==\"Micro\""], stdout=self.devnull, stderr=self.devnull)
        # Do wait for job queue to empty before restoration and reactivation
        self.poll_for_empty_job_queue(tag="Micro", interval=10, maxtime=120)

        # call base-class teardown after our class-specific work
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        # this unit test should pass
        n = 100
        sys.stdout.write("Testing submission rate over %d individual submits:\n" % (n))
        t0 = time.time()
        for k in xrange(n):
            subprocess.Popen("condor_submit", stdin=subprocess.PIPE, stdout=self.devnull, stderr=self.devnull).communicate(input='universe = vanilla\nexecutable = /bin/sleep\narguments = 10m\nrequirements = (WallabyGroups == "GridScaleTestMicro")\n+CondorUnitTestTag="Micro"\nqueue\n')
        elapsed = time.time() - t0
        sys.stdout.write("%f seconds for %d submits -> %f submissions / sec\n" % (elapsed, n, float(n)/float(elapsed)))
        


# small scale test
class grid_scale_test_small(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # I expect this from repo submodule condor_tools
        if sys.path[0] != '':
            self.ctbin = "%s/../submodules/condor_tools/bin"%(sys.path[0])
        else:
            self.ctbin = "../submodules/condor_tools/bin"


        # class specific setup goes after parent class
        provisioned_nodes = self.list_nodes(without_any_feats=['CentralManager','Negotiator','Collector'], checkin_since=time.time()-4000)
        reporting_nodes = self.reporting_nodes()
        candidate_nodes = list(set(provisioned_nodes) & set(reporting_nodes))

        self.ntarget = 8
        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestSmall')
        self.build_access_feature('GridScaleTestSmallAccess')
        self.build_execute_feature('GridScaleTestSmallExecute', n_startd=8, n_slots=8, n_dynamic=0)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTestSmall', 'GridScaleTestSmallAccess', 'GridScaleTestSmallExecute'], ['GridScaleTestSmall'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestSmall', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)
        self.clear_default_group()

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestSmall'], self.target_nodes)

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_small_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        self.poll_for_slots(self.ntarget*8*8, group='GridScaleTestSmall', interval=10, maxtime=60, required=(self.ntarget-1)*8*8, expected_nodes=self.target_nodes)

        # flag that all setup succeeded
        self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        duration = 400
        maxreps = 330
        nsub = 10
        submit_procs = []
        for j in xrange(nsub):
            cjs_command = "%s/cjs -duration %d -xgroups U%03d 1 -reqs 'stringListMember(\"GridScaleTestSmall\", WallabyGroups)' -ss -ss-interval 0.95 -ss-maxreps %d >/tmp/sh_out%03d 2>/tmp/sh_err%03d" % (self.ctbin, duration, j, maxreps, j, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        t0 = time.time()
        while True:
            time.sleep(1)
            completed = True
            for p in submit_procs:
                if p.poll() == None: completed = False
            if completed: break

        elapsed = time.time() - t0
        sys.stdout.write("elapsed time = %s  sustained rate = %f  with %d submitters\n" % (elapsed, float(maxreps)/float(elapsed), nsub))


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        cjs_command = "%s/cjs -duration 15 -n 4000 -sub 10 -reqs 'stringListMember(\"GridScaleTestSmall\", WallabyGroups)' -append '+CondorUnitTestTag=\"Small\"' >/tmp/sh_out 2>/tmp/sh_err" % (self.ctbin)
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Small", interval = 60, maxtime=600)



# medium scale test
class grid_scale_test_medium(utcondor.condor_unit_test):
    def setUp(self):
        self.setup = False
        try:
            utcondor.condor_unit_test.setUp(self)
        except:
            sys.stderr.write("setup failed")
            raise

        # I expect this from repo submodule condor_tools
        if sys.path[0] != '':
            self.ctbin = "%s/../submodules/condor_tools/bin"%(sys.path[0])
        else:
            self.ctbin = "../submodules/condor_tools/bin"

        # class specific setup goes after parent class
        qualified_nodes = self.list_nodes(without_any_feats=['CentralManager','Negotiator','Collector'], checkin_since=time.time()-4000)
        reporting_nodes = self.reporting_nodes()
        candidate_nodes = list(set(qualified_nodes) & set(reporting_nodes))

        self.ntarget = 10
        if len(candidate_nodes) < self.ntarget:
            sys.stderr.write("%d nodes insufficient for this test\n" % (len(candidate_nodes)))
            sys.stderr.write("qualified but not reporting= %s\n" % (list(set(qualified_nodes) - set(reporting_nodes))))
            sys.stderr.write("reporting but not qualified= %s\n" % (list(set(reporting_nodes) - set(qualified_nodes))))
            raise Exception()

        # just take the first ones on the list
        self.target_nodes = candidate_nodes[:self.ntarget]
        sys.stdout.write("target_nodes: %s\n" % (self.target_nodes))

        self.assert_feature('GridScaleTestMedium')
        self.build_access_feature('GridScaleTestMediumAccess')
        (pslots,dslots) = self.build_execute_feature('GridScaleTestMediumExecute', n_startd=50, n_slots=1, n_dynamic=8)

        # define features on the given groups
        self.assert_group_features(['NodeAccess', 'Master', 'GridScaleTestMedium', 'GridScaleTestMediumAccess', 'GridScaleTestMediumExecute'], ['GridScaleTestMedium'])

        # This is my subversive technique for ensuring that my target systems restart
        self.tag_test_feature('GridScaleTestMedium', 'GRID_SCALE_TEST_RESTART_TAG')

        # Make sure all config is cleared from nodes
        self.clear_nodes(self.target_nodes)
        self.clear_default_group()

        # define groups on the given nodes
        self.assert_node_groups(['GridScaleTestMedium'], self.target_nodes)

        # snapshot this test config
        self.take_snapshot("grid_scale_%s_medium_test" % (self.testdate))

        # Activate new config
        result = self.config_store.activateConfiguration(_timeout=600)
        if result.status != 0:
            raise Exception("Failed to activate test configuration: (%s, %s)" % (result.status, result.text))

        # before we leave set-up, make sure activation and restart are complete
        try:
            self.poll_for_slots(self.ntarget*pslots, group='GridScaleTestMedium', interval=15, maxtime=300, expected_nodes=self.target_nodes)
        except:
            pass
        else:
            # flag that all setup succeeded
            self.setup = True


    def tearDown(self):
        # class specific teardown goes before parent class
        utcondor.condor_unit_test.tearDown(self)


    def test_submit_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        duration = 15
        sustain = 300
        nsub = 20
        interval = 0.0
        submit_procs = []
        sincetime=time.time()
        for j in xrange(nsub):
            cjs_command = "%s/cjs -duration %d -xgroups U%03d 1 -reqs 'stringListMember(\"GridScaleTestMedium\", WallabyGroups)' -ss -ss-interval %f -ss-maxtime %d -log -append '+CondorUnitTestTag=\"Medium\"' >/tmp/sh_out%03d 2>/tmp/sh_err%03d" % (self.ctbin, duration, j, interval, sustain, j, j)
            sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
            proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
            submit_procs += [proc]

        sys.stdout.write("Waiting for spawned submission processes to complete...\n")
        elapsed = self.poll_for_process_completion(submit_procs)

        njobs = self.job_count(tag="Medium")
        sys.stdout.write("elapsed time = %s  sustained rate = %f  with %d submitters\n" % (elapsed, float(njobs)/float(elapsed), nsub))

        self.remove_jobs(tag="Medium")
        self.poll_for_empty_job_queue(tag="Medium", interval=15, maxtime=3600)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("submissions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions"%(self.ctbin, hfname, int(sincetime))])
        sys.stdout.write("submissions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -submissions -cum -rate"%(self.ctbin, hfname, int(sincetime))])
        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(self.ctbin, hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(self.ctbin, hfname, int(sincetime))])


    def test_completion_rate(self):
        if self.params.setup_only: return
        
        if not self.setup:
            sys.stderr.write("setup failed")
            raise Exception()
        # this unit test should pass
        cjs_command = "%s/cjs -duration 30 -n 10000 -sub 20 -reqs 'stringListMember(\"GridScaleTestMedium\", WallabyGroups)' -append '+CondorUnitTestTag=\"Medium\"' >/tmp/sh_out 2>/tmp/sh_err" % (self.ctbin)

        sincetime=time.time()
        sys.stdout.write("spawning submit process \"%s\"\n" % (cjs_command))
        proc = subprocess.Popen(["/bin/sh", "-c", cjs_command], stdout=self.devnull, stderr=self.devnull)
        proc.wait()

        # this waits for jobs to finish, and also measures completion rate
        self.poll_for_empty_job_queue(tag = "Medium", interval = 60, maxtime=3600)

        hfname = tempfile.mktemp(prefix="sh_hist_")
        subprocess.call(["/bin/sh", "-c", "/usr/sbin/condor_fetchlog %s HISTORY > %s"%(self.params.broker_addr, hfname)])

        sys.stdout.write("completions:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d"%(self.ctbin, hfname, int(sincetime))])
        sys.stdout.write("completions cum rate:\n")
        subprocess.call(["/bin/sh", "-c", "%s/plot_pool_thruput -noplot -timeslice 30 -f %s -since %d -cum -rate"%(self.ctbin, hfname, int(sincetime))])


# inherit standard args from utcondor
ha_parser = argparse.ArgumentParser(parents=[utcondor.parser])
ha_parser.add_argument('--setup-only', action='store_true', default=False, help='run test setup only: skip tests and do not restore config')
# Tentatively, I don't think it's a good idea to run all test cases
# So I'm making this a required positional param
ha_parser.add_argument('test_name')

# parse args from command line
args = ha_parser.parse_args()

# initialize utcondor params
if args.setup_only: args.no_restore = True
utcondor.init(args)

unittest.main(argv=[sys.argv[0], args.test_name])
